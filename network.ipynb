{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd86f56d-0b4f-4fab-9d97-68405cf00a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe190e4-d54b-41e2-a4e4-aea53c1379b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ca371c-31c5-46fe-8ce1-6c7f4be3d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Opaque\", \"Red\", \"Green\", \"Yellow\", \"Bright\", \"Light-blue\", \"Colors\", \"Red\", \"Women\", \"Enemy\", \"Son\", \"Man\", \"Away\", \"Drawer\", \"Born\", \"Learn\",\n",
    "          \"Call\", \"Skimmer\", \"Bitter\", \"Sweet milk\", \"Milk\", \"Water\", \"Food\", \"Argentina\", \"Uruguay\", \"Country\", \"Last name\", \"Where\", \"Mock\", \"Birthday\", \"Breakfast\", \"Photo\",\n",
    "          \"Hungry\", \"Map\", \"Coin\", \"Music\", \"Ship\", \"None\", \"Name\", \"Patience\", \"Perfume\", \"Deaf\", \"Trap\", \"Rice\", \"Barbecue\", \"Candy\", \"Chewing-gum\", \"Spaghetti\",\n",
    "          \"Yogurt\", \"Accept\", \"Thanks\", \"Shut down\", \"Appear\", \"To land\", \"Catch\", \"Help\", \"Dance\", \"Bathe\", \"Buy\", \"Copy\", \"Run\", \"Realize\", \"Give\", \"Find\"]\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'dataset', 'npz')\n",
    "datasets = os.listdir(data_dir)\n",
    "selected_frames = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76a4488-5318-4eaf-8d01-b5493de56c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for filename in datasets:\n",
    "    content = np.load(os.path.join(data_dir, filename))\n",
    "    length = content['x'].shape[0]\n",
    "    if length < selected_frames:\n",
    "        continue\n",
    "    selected = content['y'][np.linspace(0, length-1, selected_frames).astype(int)]\n",
    "    # x = [line.flatten() for line in selected]\n",
    "    X.append(np.array(selected, dtype='float32'))\n",
    "    y.append(int(filename.split('_')[0])-1)\n",
    "\n",
    "X = np.array(X, dtype='float32')\n",
    "y = np.array(y, dtype='uint8')\n",
    "y = to_categorical(y).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe517c5e-23f3-429a-9dc2-650ba3f74693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((2611, 16, 75, 3), (2611, 64))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20e437b6-f1d8-40c5-a22e-965159463a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "callback = TensorBoard(log_dir)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658778aa-7065-44d8-8f0b-bb3a20b0c96b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d240bb18-894a-471e-b960-5c1ff599fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdn(att_lstm_num, att_lstm_seq_len, lstm_seq_len, feature_vec_len, cnn_flat_size = 128, lstm_out_size = 128, nbhd_size = 3, \n",
    "         nbhd_type = 2, map_x_num = 10, map_y_num = 20, flow_type = 4, output_shape = 2, optimizer = 'adagrad', loss = 'mse', metrics=[]):\n",
    "\n",
    "        flatten_att_nbhd_inputs = [Input(shape = (nbhd_size, nbhd_size, nbhd_type,), name = \"att_nbhd_volume_input_time_{0}_{1}\".format(att+1, ts+1)) for ts in range(att_lstm_seq_len) for att in range(att_lstm_num)]\n",
    "        flatten_att_flow_inputs = [Input(shape = (nbhd_size, nbhd_size, flow_type,), name = \"att_flow_volume_input_time_{0}_{1}\".format(att+1, ts+1)) for ts in range(att_lstm_seq_len) for att in range(att_lstm_num)]\n",
    "\n",
    "        att_nbhd_inputs = []\n",
    "        att_flow_inputs = []\n",
    "        for att in range(att_lstm_num):\n",
    "            att_nbhd_inputs.append(flatten_att_nbhd_inputs[att*att_lstm_seq_len:(att+1)*att_lstm_seq_len])\n",
    "            att_flow_inputs.append(flatten_att_flow_inputs[att*att_lstm_seq_len:(att+1)*att_lstm_seq_len])\n",
    "\n",
    "        att_lstm_inputs = [Input(shape = (att_lstm_seq_len, feature_vec_len,), name = \"att_lstm_input_{0}\".format(att+1)) for att in range(att_lstm_num)]\n",
    "        nbhd_inputs = [Input(shape = (nbhd_size, nbhd_size, nbhd_type,), name = \"nbhd_volume_input_time_{0}\".format(ts+1)) for ts in range(lstm_seq_len)]\n",
    "        flow_inputs = [Input(shape = (nbhd_size, nbhd_size, flow_type,), name = \"flow_volume_input_time_{0}\".format(ts+1)) for ts in range(lstm_seq_len)]\n",
    "        lstm_inputs = Input(shape = (lstm_seq_len, feature_vec_len,), name = \"lstm_input\")\n",
    "\n",
    "        #short-term part\n",
    "        #1st level gate\n",
    "        #nbhd cnn\n",
    "        nbhd_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"nbhd_convs_time0_{0}\".format(ts+1))(nbhd_inputs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [Activation(\"relu\", name = \"nbhd_convs_activation_time0_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        #flow cnn\n",
    "        flow_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"flow_convs_time0_{0}\".format(ts+1))(flow_inputs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_convs = [Activation(\"relu\", name = \"flow_convs_activation_time0_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        #flow gate\n",
    "        flow_gates = [Activation(\"sigmoid\", name = \"flow_gate0_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [keras.layers.Multiply()([nbhd_convs[ts], flow_gates[ts]]) for ts in range(lstm_seq_len)]\n",
    "\n",
    "\n",
    "        #2nd level gate\n",
    "        nbhd_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"nbhd_convs_time1_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [Activation(\"relu\", name = \"nbhd_convs_activation_time1_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"flow_convs_time1_{0}\".format(ts+1))(flow_inputs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_convs = [Activation(\"relu\", name = \"flow_convs_activation_time1_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_gates = [Activation(\"sigmoid\", name = \"flow_gate1_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [keras.layers.Multiply()([nbhd_convs[ts], flow_gates[ts]]) for ts in range(lstm_seq_len)]\n",
    "\n",
    "        #3rd level gate\n",
    "        nbhd_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"nbhd_convs_time2_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [Activation(\"relu\", name = \"nbhd_convs_activation_time2_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_convs = [Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"flow_convs_time2_{0}\".format(ts+1))(flow_inputs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_convs = [Activation(\"relu\", name = \"flow_convs_activation_time2_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        flow_gates = [Activation(\"sigmoid\", name = \"flow_gate2_{0}\".format(ts+1))(flow_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_convs = [keras.layers.Multiply()([nbhd_convs[ts], flow_gates[ts]]) for ts in range(lstm_seq_len)]\n",
    "\n",
    "\n",
    "        #dense part\n",
    "        nbhd_vecs = [Flatten(name = \"nbhd_flatten_time_{0}\".format(ts+1))(nbhd_convs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_vecs = [Dense(units = cnn_flat_size, name = \"nbhd_dense_time_{0}\".format(ts+1))(nbhd_vecs[ts]) for ts in range(lstm_seq_len)]\n",
    "        nbhd_vecs = [Activation(\"relu\", name = \"nbhd_dense_activation_time_{0}\".format(ts+1))(nbhd_vecs[ts]) for ts in range(lstm_seq_len)]\n",
    "\n",
    "        #feature concatenate\n",
    "        nbhd_vec = Concatenate(axis=-1)(nbhd_vecs)\n",
    "        nbhd_vec = Reshape(target_shape = (lstm_seq_len, cnn_flat_size))(nbhd_vec)\n",
    "        lstm_input = Concatenate(axis=-1)([lstm_inputs, nbhd_vec])\n",
    "\n",
    "        #lstm\n",
    "        lstm = LSTM(units=lstm_out_size, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)(lstm_input)\n",
    "\n",
    "        #attention part\n",
    "        att_nbhd_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_nbhd_convs_time0_{0}_{1}\".format(att+1,ts+1))(att_nbhd_inputs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[Activation(\"relu\", name = \"att_nbhd_convs_activation_time0_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_flow_convs_time0_{0}_{1}\".format(att+1,ts+1))(att_flow_inputs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Activation(\"relu\", name = \"att_flow_convs_activation_time0_{0}_{1}\".format(att+1,ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_gates = [[Activation(\"sigmoid\", name = \"att_flow_gate0_{0}_{1}\".format(att+1, ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[keras.layers.Multiply()([att_nbhd_convs[att][ts], att_flow_gates[att][ts]]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "\n",
    "        att_nbhd_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_nbhd_convs_time1_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[Activation(\"relu\", name = \"att_nbhd_convs_activation_time1_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_flow_convs_time1_{0}_{1}\".format(att+1,ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Activation(\"relu\", name = \"att_flow_convs_activation_time1_{0}_{1}\".format(att+1,ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_gates = [[Activation(\"sigmoid\", name = \"att_flow_gate1_{0}_{1}\".format(att+1, ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[keras.layers.Multiply()([att_nbhd_convs[att][ts], att_flow_gates[att][ts]]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "\n",
    "        att_nbhd_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_nbhd_convs_time2_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[Activation(\"relu\", name = \"att_nbhd_convs_activation_time2_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Conv2D(filters = 64, kernel_size = (3,3), padding=\"same\", name = \"att_flow_convs_time2_{0}_{1}\".format(att+1,ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_convs = [[Activation(\"relu\", name = \"att_flow_convs_activation_time2_{0}_{1}\".format(att+1,ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_flow_gates = [[Activation(\"sigmoid\", name = \"att_flow_gate2_{0}_{1}\".format(att+1, ts+1))(att_flow_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_convs = [[keras.layers.Multiply()([att_nbhd_convs[att][ts], att_flow_gates[att][ts]]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "\n",
    "        att_nbhd_vecs = [[Flatten(name = \"att_nbhd_flatten_time_{0}_{1}\".format(att+1,ts+1))(att_nbhd_convs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_vecs = [[Dense(units = cnn_flat_size, name = \"att_nbhd_dense_time_{0}_{1}\".format(att+1,ts+1))(att_nbhd_vecs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "        att_nbhd_vecs = [[Activation(\"relu\", name = \"att_nbhd_dense_activation_time_{0}_{1}\".format(att+1,ts+1))(att_nbhd_vecs[att][ts]) for ts in range(att_lstm_seq_len)] for att in range(att_lstm_num)]\n",
    "\n",
    "\n",
    "        att_nbhd_vec = [Concatenate(axis=-1)(att_nbhd_vecs[att]) for att in range(att_lstm_num)]\n",
    "        att_nbhd_vec = [Reshape(target_shape = (att_lstm_seq_len, cnn_flat_size))(att_nbhd_vec[att]) for att in range(att_lstm_num)]\n",
    "        att_lstm_input = [Concatenate(axis=-1)([att_lstm_inputs[att], att_nbhd_vec[att]]) for att in range(att_lstm_num)]\n",
    "\n",
    "        att_lstms = [LSTM(units=lstm_out_size, return_sequences=True, dropout=0.1, recurrent_dropout=0.1, name=\"att_lstm_{0}\".format(att + 1))(att_lstm_input[att]) for att in range(att_lstm_num)]\n",
    "\n",
    "        #compare\n",
    "        att_low_level=[attention.Attention(method='cba')([att_lstms[att], lstm]) for att in range(att_lstm_num)]\n",
    "        att_low_level=Concatenate(axis=-1)(att_low_level)\n",
    "        att_low_level=Reshape(target_shape=(att_lstm_num, lstm_out_size))(att_low_level)\n",
    "\n",
    "\n",
    "        att_high_level = LSTM(units=lstm_out_size, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)(att_low_level)\n",
    "\n",
    "        lstm_all = Concatenate(axis=-1)([att_high_level, lstm])\n",
    "        # lstm_all = Dropout(rate = .3)(lstm_all)\n",
    "        lstm_all = Dense(units = output_shape)(lstm_all)\n",
    "        pred_volume = Activation('tanh')(lstm_all)\n",
    "\n",
    "        inputs = flatten_att_nbhd_inputs + flatten_att_flow_inputs + att_lstm_inputs + nbhd_inputs + flow_inputs + [lstm_inputs,]\n",
    "        # print(\"Model input length: {0}\".format(len(inputs)))\n",
    "        model = Model(inputs = inputs, outputs = pred_volume)\n",
    "        model.compile(optimizer = optimizer, loss = loss, metrics=metrics)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c2c89-aa76-4f7b-b9a3-80bce0b275be",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07062ab5-b32f-44ea-9d3c-29c863e26dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=X_train.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# (CONV => RELU) * 2 => POOL\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    " \n",
    "# softmax classifier\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9c1278-ab4c-4fac-b6c1-ad50e65763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3497cea3-f1c7-44dd-a29d-001b2711b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "82/82 [==============================] - 5s 43ms/step - loss: 4.3606 - accuracy: 0.0839\n",
      "Epoch 2/40\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 2.7583 - accuracy: 0.2761\n",
      "Epoch 3/40\n",
      "82/82 [==============================] - 3s 42ms/step - loss: 1.9275 - accuracy: 0.4424\n",
      "Epoch 4/40\n",
      "82/82 [==============================] - 4s 44ms/step - loss: 1.4812 - accuracy: 0.5523\n",
      "Epoch 5/40\n",
      "82/82 [==============================] - 4s 44ms/step - loss: 1.1413 - accuracy: 0.6385\n",
      "Epoch 6/40\n",
      "82/82 [==============================] - 4s 43ms/step - loss: 0.9129 - accuracy: 0.6951\n",
      "Epoch 7/40\n",
      "82/82 [==============================] - 4s 44ms/step - loss: 0.8290 - accuracy: 0.7250\n",
      "Epoch 8/40\n",
      "82/82 [==============================] - 4s 44ms/step - loss: 0.7243 - accuracy: 0.7579\n",
      "Epoch 9/40\n",
      "82/82 [==============================] - 3s 43ms/step - loss: 0.6546 - accuracy: 0.7901\n",
      "Epoch 10/40\n",
      "82/82 [==============================] - 4s 43ms/step - loss: 0.5518 - accuracy: 0.8089\n",
      "Epoch 11/40\n",
      "82/82 [==============================] - 4s 45ms/step - loss: 0.5063 - accuracy: 0.8307\n",
      "Epoch 12/40\n",
      "82/82 [==============================] - 4s 46ms/step - loss: 0.4700 - accuracy: 0.8480\n",
      "Epoch 13/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.4176 - accuracy: 0.8606\n",
      "Epoch 14/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.4371 - accuracy: 0.8522\n",
      "Epoch 15/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.3997 - accuracy: 0.8594\n",
      "Epoch 16/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.3539 - accuracy: 0.8755\n",
      "Epoch 17/40\n",
      "82/82 [==============================] - 4s 52ms/step - loss: 0.3286 - accuracy: 0.8805\n",
      "Epoch 18/40\n",
      "82/82 [==============================] - 4s 51ms/step - loss: 0.2870 - accuracy: 0.8997\n",
      "Epoch 19/40\n",
      "82/82 [==============================] - 4s 50ms/step - loss: 0.2933 - accuracy: 0.9012\n",
      "Epoch 20/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.2879 - accuracy: 0.9012\n",
      "Epoch 21/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.3211 - accuracy: 0.8878\n",
      "Epoch 22/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.2721 - accuracy: 0.9016\n",
      "Epoch 23/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.2214 - accuracy: 0.9230\n",
      "Epoch 24/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.2147 - accuracy: 0.9200\n",
      "Epoch 25/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.2100 - accuracy: 0.9288\n",
      "Epoch 26/40\n",
      "82/82 [==============================] - 4s 49ms/step - loss: 0.1992 - accuracy: 0.9288\n",
      "Epoch 27/40\n",
      "82/82 [==============================] - 4s 50ms/step - loss: 0.1931 - accuracy: 0.9314\n",
      "Epoch 28/40\n",
      "82/82 [==============================] - 4s 49ms/step - loss: 0.1943 - accuracy: 0.9318\n",
      "Epoch 29/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1574 - accuracy: 0.9448\n",
      "Epoch 30/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1722 - accuracy: 0.9399\n",
      "Epoch 31/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1566 - accuracy: 0.9426\n",
      "Epoch 32/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1703 - accuracy: 0.9422\n",
      "Epoch 33/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1676 - accuracy: 0.9452\n",
      "Epoch 34/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.1495 - accuracy: 0.9452\n",
      "Epoch 35/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.1656 - accuracy: 0.9406\n",
      "Epoch 36/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.1804 - accuracy: 0.9437\n",
      "Epoch 37/40\n",
      "82/82 [==============================] - 4s 47ms/step - loss: 0.1587 - accuracy: 0.9433\n",
      "Epoch 38/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1339 - accuracy: 0.9544\n",
      "Epoch 39/40\n",
      "82/82 [==============================] - 4s 49ms/step - loss: 0.1280 - accuracy: 0.9548\n",
      "Epoch 40/40\n",
      "82/82 [==============================] - 4s 48ms/step - loss: 0.1250 - accuracy: 0.9579\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2bf84c74df0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50888b37-db58-4276-92c2-bbdd9804425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/VGGNet15\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model/VGGNet15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccab5948-784a-4dbc-ae55-087e4733faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/VGGNet-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e52733-a631-4965-9247-686f55566ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7eab163d-65d6-482d-a829-172e37f2fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b12fad9a-3f4f-4dc2-887d-f359c1bc079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, -28,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  -5,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       -25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   8,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, -11,   0,  -2,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, -33,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  -3,  -2,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, -28,   0,   0,   7,   0,   0,   0,\n",
       "         0,   0,  34,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p, axis=1) - np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a8838-b1cd-4e7d-a67a-479f51959b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Input(shape=X_test.shape[1:]),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    \n",
    ")\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(256, activation='relu', return_sequences=True))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=False))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['categorical_accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb6381-7a4b-4826-a78a-ec3054661424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=20)#, callbacks=[callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8415968c-f50e-446a-b872-b28c5ef64154",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990a119-1ae5-4a52-8d77-721e7fd364a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_predict, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}